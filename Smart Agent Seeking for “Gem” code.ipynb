{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NU.png\" width =\"300\" height=300>\n",
    "<h1 style=\"text-align:center\">An-Najah National University</h1>\n",
    "<h1 style=\"text-align:center\">Artificial Intelligence</h1>\n",
    "&nbsp;\n",
    "<h1 style=\"text-align:center;color:blue\"> Smart Agent Seeking for “Gem”</h1>\n",
    "&nbsp;\n",
    "<h1 style=\"text-align:center\">By: Sabri Odeh</h1>\n",
    "<h1 style=\"text-align:center\">Supervisor: Dr. Hamed Abdelhaq</h1>\n",
    "&nbsp;\n",
    "<h1 style=\"text-align:center\">19 April 2021</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the shape of the environment\n",
    "def environment_shape(lw):\n",
    "    #Create an array (2D) have the rewards for each state. \n",
    "    #The array contains 3 rows and n columns\n",
    "    #each value is initialized to -100.\n",
    "    rewards = np.full((environment_rows, environment_columns), -100.)\n",
    "    \n",
    "    #set the rewards for all paths cells (grey squares)\n",
    "    pc = {} #store locations in a dictionary\n",
    "    pc[1] = [i for i in range(1, environment_columns-1)] \n",
    "    row_index =1\n",
    "    for column_index in pc[row_index]:\n",
    "         rewards[row_index, column_index] = lw\n",
    "            \n",
    "    #set corners of the environment to 0 \n",
    "    rewards[0, 0] = 0\n",
    "    rewards[0, environment_columns-1] = 0\n",
    "    rewards[2, 0] = 0\n",
    "    rewards[2, environment_columns-1] = 0\n",
    "    \n",
    "    #set the reward for the squares goal to 100 \n",
    "    #squares goal have random locations\n",
    "    while(1):\n",
    "        x= random.randrange(3)\n",
    "        y=  random.sample(range(1, environment_columns-1), 1)\n",
    "        if (x!=1 and y!=[i for i in range(1, environment_columns-1)]):\n",
    "            break \n",
    "    rewards[x ,y] = 100 \n",
    "\n",
    "    while(1):\n",
    "        x= random.randrange(3)\n",
    "        y=  random.sample(range(1, environment_columns-1), 1)\n",
    "        if (x!=1 and y!=[i for i in range(1, environment_columns-1)]):\n",
    "            break\n",
    "    rewards[x , y] = 100\n",
    "   \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  #if the reward for this location is lw value, then it is grey squares\n",
    "  if rewards[current_row_index, current_column_index] == lw:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "#define a function that will choose a random grey square location \n",
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_columns)\n",
    "  #continue choosing random row and column indexes until a grey square state is identified\n",
    "    while is_terminal_state(current_row_index, current_column_index):\n",
    "        current_row_index = np.random.randint(environment_rows)\n",
    "        current_column_index = np.random.randint(environment_columns)\n",
    "    return current_row_index, current_column_index\n",
    "\n",
    "#define an epsilon greedy algorithm that will choose which action to take next (where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
    "  #then choose the most promising value from the Q-table for this state.\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else: #choose a random action\n",
    "        return np.random.randint(4)\n",
    "\n",
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'north ' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'south ' and current_row_index < environment_rows - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    return new_row_index, new_column_index\n",
    "\n",
    "#Define a function that will get the shortest path between any location within the environment\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  #return immediately if this is an invalid starting location\n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    else: #if this is a 'legal' starting location\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "    #continue moving along the path until we reach the goal\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      #get the best action to take\n",
    "        action_index = get_next_action(current_row_index, current_column_index, 0.7)\n",
    "      #move to the next location on the path, and add the new location to the list\n",
    "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(discount_factor,learning_rate,epoch):\n",
    "\n",
    "    epsilon = 0.7 #the percentage of time when we should take the best action (instead of a random action)\n",
    "\n",
    "    for episode in range(epoch):\n",
    "      #get the starting location for this episode\n",
    "        row_index, column_index = get_starting_location()\n",
    "\n",
    "      #continue taking actions (moving) until we reach a terminal state\n",
    "        while not is_terminal_state(row_index, column_index):\n",
    "            #choose which action to take (where to move next)\n",
    "            action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "            #perform the chosen action, and transition to the next state (move to the next location)\n",
    "            old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "            row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "            #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "            reward = rewards[row_index, column_index]\n",
    "            old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "            temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "            #update the Q-value for the previous state and action pair\n",
    "            new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "            q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The width of the environment:\n",
      "6\n",
      "###########################################################\n",
      "The Living rewards is:\n",
      "-1\n",
      "[   0. -100. -100.  100. -100.    0.]\n",
      "[-100.   -1.   -1.   -1.   -1. -100.]\n",
      "[   0. -100. -100. -100.  100.    0.]\n",
      "###########################################################\n",
      "The discount_factor is:\n",
      "0.9\n",
      "The learning_rate is:\n",
      "0.9\n",
      "The number of episodes is:\n",
      "1000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "environment_rows = 3\n",
    "print('The width of the environment:')\n",
    "environment_columns = int(input())\n",
    "#Create an array (3D) to hold the current Q-values for each state and action pair: Q(s, a) \n",
    "#The value of each (state, action) pair is initialized to 0.\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
    "\n",
    "#define actions\n",
    "#numeric action codes: 0 = north, 1 = right, 2 = south, 3 = left\n",
    "actions = ['north ', 'right', 'south ', 'left']\n",
    "\n",
    "print('###########################################################')\n",
    "print('The Living rewards is:')\n",
    "lw = int(input())\n",
    "rewards=environment_shape(lw)\n",
    "#print rewards matrix\n",
    "for row in rewards:\n",
    "    print(row)\n",
    "print('###########################################################')\n",
    "\n",
    "#enter training parameters\n",
    "print('The discount_factor is:')\n",
    "discount_factor = float(input())\n",
    "\n",
    "print('The learning_rate is:')\n",
    "learning_rate = float(input())\n",
    "\n",
    "print('The number of episodes is:')\n",
    "epoch = int(input())\n",
    "\n",
    "Training(discount_factor,learning_rate,epoch)\n",
    "print('Training complete!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north</th>\n",
       "      <th>right</th>\n",
       "      <th>south</th>\n",
       "      <th>left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>79.1</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>70.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>79.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>89.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north  right  south    left\n",
       "0    0.0    0.0    0.0    0.00\n",
       "1 -100.0   79.1 -100.0 -100.00\n",
       "2 -100.0   89.0 -100.0   70.19\n",
       "3  100.0   89.0 -100.0   79.10\n",
       "4 -100.0 -100.0  100.0   89.00\n",
       "5    0.0    0.0    0.0    0.00"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display a Q_value Tabel\n",
    "df = pd.DataFrame(q_values[1], columns = ['north','right','south','left'])#index => 0=S0, 1= S1, 2= S2 .....\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [1, 3], [0, 3]]\n"
     ]
    }
   ],
   "source": [
    "#display a few shortest paths\n",
    "print(get_shortest_path(1, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
